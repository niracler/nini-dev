# 页 17：Context Window 的生命周期（🔴 重点）

## 页面内容摘要

```text
# Context Window 的生命周期
🔴 重点

左栏：Context 膨胀动画
- 固定层：System 5K + Tools 3K + MCP 2K
- 动态增长：用户请求 → 读文件 +3K → AI 回复 +2K → 编辑测试 +8K → 安全检查 +25K → 继续迭代 +120K
- 95% 时触发 Auto Compact

右栏：Compact 后效果
- 从 95% → 18%
- 摘要替代详细历史（~8K）
- 可用空间 ~180K

最佳实践：在逻辑断点手动 /compact，而非等待 95%

脚注：
- LLM 是无状态的，每次请求都带上完整 context
- Prompt Caching：厂商缓存重复前缀降成本（Anthropic 10%、OpenAI 50%、Google 25%）
```

## 6 问分析

### 1. 如何讲

- **时长**：~3 分钟
- **节奏**：跟着 v-click 动画逐步讲解，让听众「看到」context 膨胀过程
- **过渡语**：
  - 开场：「刚才讲了 Subagent 和 Skill 两种策略。但 Context 终究会满——现在看它的完整生命周期」
  - 结束：「理解了 Context 生命周期，接下来看如何主动管理它——Context Engineering」
- **核心记忆点**：**95% 自动压缩——但在逻辑断点手动 /compact 更好**

### 2. 为何要懂

- **重要程度**：🔴 必须懂
- **价值**：这是日常使用 Claude Code 最容易遇到的问题——对话久了变慢、回答质量下降
- **与听众关联**：
  - 理解为什么长对话后 AI 表现变差（Context 接近极限）
  - 知道何时该 /compact，而不是被动等待

**真实案例**（[HN 讨论](https://news.ycombinator.com/item?id=46237160)）：

> 有开发者抱怨 LLM 回答质量差。一问才知道——他们在同一个对话里问食谱、聊个人问题、然后写代码。整个 context 都混在一起，AI 当然懵了。

**教训**：不同话题开不同对话，或者在切换任务时 /compact。这就是为什么理解 context 机制很重要。

### 3. 演示策略

- **需要演示**：是（简短演示）
- **演示方式**：在 Claude Code 中输入 `/context`，展示当前 context 使用情况
- **演示要点**：
  - 让听众看到实际的 context 使用百分比
  - 展示 System、Tools、Conversation 各占多少
  - 如果当前对话已经有一定长度，效果更好
- **时长**：~15 秒
- **补充口述**：「我一般在完成一个功能后 /compact，相当于『存档』」

### 4. 可能问题

| 问题 | 准备的回答 |
| ------------------------- | --------------------------------------------------------------------------------------------------- |
| Compact 会丢失信息吗？ | 会压缩详细过程，但保留关键决策、代码变更、进行中任务。重要结论不会丢 |
| 什么时候该手动 /compact？ | 在逻辑断点——完成一个功能、解决一个 bug、准备切换任务时。相当于「保存进度」 |
| LLM 无状态是什么意思？ | 模型没有「记忆」。每次请求都把整个对话历史发过去，模型重新读一遍再回答。这就是为什么 context 会膨胀 |
| Prompt Caching 省多少钱？ | 各厂商不同。Anthropic 缓存命中只收 10%，OpenAI 50%，Google 25%。长对话越多越省 |
| 95% 为什么是临界点？ | 研究表明 LLM 在接近 context 限制时表现恶化。剩余空间是「工作记忆」，太少了推理质量下降 |

### 5. 取舍逻辑

| 没讲的内容 | 取舍理由 |
| --------------------------- | ------------------------------- |
| Compact 算法细节 | 📚 太底层，用户不需要知道 |
| 不同模型的 context 限制对比 | ⏱️ 变化快，查最新文档 |
| Context 管理的高级技巧 | 下一页 Context Engineering 会讲 |

### 6. 观点/事实区分

| 内容 | 类型 | 来源 |
| ---------------------------------- | -------- | -------------------- |
| 95% 时自动 compact | 事实 | Claude Code 行为 |
| LLM 无状态，每次带完整 context | 事实 | LLM 架构原理 |
| Prompt Caching 价格（10%/50%/25%） | 事实 | 各厂商官方文档 |
| 在逻辑断点手动 compact 更好 | 最佳实践 | Claude Code 使用经验 |

## 演示备忘

**演示命令**：

```bash
/context
```

**演示时口述**：

> 「我输入 /context 看看当前的使用情况。你们看——System 占多少、Tools 占多少、对话内容占多少。现在用了 X%，还有很多空间。但如果继续聊下去，就会越来越满。」

**口述重点**：

> 「每次交互都在增加 context。到 95% 时自动压缩。但我建议不要等到 95%——在完成一个功能后主动 /compact，相当于『存档』。」

**个人经验分享**：

> 「我自己的习惯：完成一个功能就 /compact 一次。这样既保留了关键信息，又腾出空间给下一个任务。比被动等 95% 更可控。」
