# 页 11：RAG 检索增强生成（⏩ 快速带过）

## 页面内容摘要

```text
# RAG 检索增强生成
⏩ 快速带过
2020 · 与其让模型记住所有知识，不如在需要时「去查」

Mermaid 流程图：
- 索引阶段：文档库 → Embedding Model → 向量数据库
- 查询阶段：query → Embedding → 向量搜索 → Top-K 相关上下文 → LLM → answer

解决的问题：✅ 知识截止 ✅ 减少幻觉
典型应用：Cursor @codebase、NotebookLM
```

## 6 问分析

### 1. 如何讲

- **时长**：~1.5 分钟
- **节奏**：快速过流程图，重点讲「解决的问题」和「典型应用」
- **过渡语**：
  - 开场：「Function Call 让 AI 能调用工具，但如何让它知道『该查什么』？RAG 就是答案」
  - 结束：「RAG 解决了知识问题，接下来看如何让 AI『想得更清楚』——思维链」
- **核心记忆点**：**与其让模型记住所有知识，不如在需要时「去查」**

### 2. 为何要懂

- **重要程度**：🟡 懂了更好
- **价值**：理解 Cursor @codebase 为什么能理解整个项目的原理

### 3. 演示策略

- **需要演示**：否（流程图已足够清晰）

### 4. 可能问题

| 问题 | 准备的回答 |
| ----------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- |
| 向量数据库是什么？ | 存储「语义」而不是「关键词」的数据库。「猫」和「喵星人」在向量空间里很近，能被一起检索出来 |
| 和传统搜索有什么区别？ | 传统搜索是关键词匹配；RAG 是语义匹配。搜「如何修复登录问题」能找到标题是「认证失败解决方案」的文档 |
| Cursor @codebase 就是 RAG？ | 是的。它把你的代码库索引成向量，提问时先检索相关代码，再喂给 LLM |
| 能完全消除幻觉吗？ | 不能，只能减少。如果检索到的内容本身有错，或者检索不全，仍可能幻觉 |
| 为什么 Claude Code 不用 RAG？ | 因为 Context 够大了（200K+），可以按需读取文件。RAG 是「先索引再检索」，Claude Code 是「理解任务后按需读取」——更灵活、更实时。但超大代码库仍需要 RAG |

### 5. 取舍逻辑

快速带过，不深入 Embedding、分块策略、重排序等技术细节。重点是理解「查了再答」的核心思想。

**技术演进对比**（如果被问到）：

| 因素 | RAG（2020） | 现代 Agent（2025） |
| ------------ | ------------------- | ------------------------------ |
| Context 大小 | 2K-8K，必须精选 | 200K+，能装下大量代码 |
| 检索方式 | 预先索引 → 向量搜索 | 按需使用工具（Glob/Grep/Read） |
| 实时性 | 索引可能过时 | 每次都读最新文件 |
| 灵活性 | 依赖索引质量 | 根据任务动态决定读什么 |

### 6. 观点/事实区分

| 内容 | 类型 | 来源 |
| ------------------------- | -------- | ---------------- |
| 2020 年提出 | 事实 | Meta AI RAG 论文 |
| 解决知识截止和幻觉 | 共识观点 | 行业普遍认同 |
| Cursor 使用 RAG | 事实 | Cursor 官方文档 |
| 现代 Agent 更多用按需读取 | 共识观点 | 工具实践 |
